{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import load_model\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = 1000  # Size of the validation set\n",
    "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 24  # Maximum number of words in a sequence\n",
    "GLOVE_DIM = 50  # Number of dimensions of the GloVe word embeddings\n",
    "INPUT_PATH = '../input'  # Path where all input files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('./')\n",
    "input_path = root / 'input/' \n",
    "ouput_path = root / 'output/'\n",
    "source_path = root / 'source/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=NB_START_EPOCHS\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , validation_data=(X_valid, y_valid)\n",
    "              , verbose=1)\n",
    "    model.save(\"./output/model/model.h5\")\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(history, metric_name):\n",
    "    '''\n",
    "    Function to evaluate a trained model on a chosen metric. \n",
    "    Training and validation metric are plotted in a\n",
    "    line chart for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        history : model training history\n",
    "        metric_name : loss or accuracy\n",
    "    Output:\n",
    "        line chart with epochs of x-axis and metric on\n",
    "        y-axis\n",
    "    '''\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "#     emb_results2.save(\"./output/model/model.h5\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    '''\n",
    "    Function to remove English stopwords from a Pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(input_text):\n",
    "    '''\n",
    "    Function to remove mentions, preceded by @, in a Pandas Series\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    return re.sub(r'@\\w+', '', input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell-3060/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "tweets_dir = \"twitter-airline-sentiment/\"\n",
    "df = pd.read_csv(input_path / 'train.csv')\n",
    "df = df.reindex(np.random.permutation(df.index))  \n",
    "df = df[['comment_text', 'toxic']]\n",
    "df.text = df.comment_text.apply(remove_stopwords).apply(remove_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: 143613\n",
      "# Test data samples: 15957\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.comment_text, df.toxic, test_size=0.1, random_state=37)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    143613.000000\n",
       "mean         68.005564\n",
       "std         100.886617\n",
       "min           1.000000\n",
       "25%          17.000000\n",
       "50%          36.000000\n",
       "75%          76.000000\n",
       "max        2273.000000\n",
       "Name: comment_text, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
    "seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2015,   36,  627,   10,  192,   28, 1647,   35,    4,   14,    1,\n",
       "         75,   62,   18, 3231,    9,  944,    7,  646,    9,    8,   70,\n",
       "         28,    8], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_trunc[10]  # Example of padded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (14362, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb.shape[0] == y_valid_emb.shape[0]\n",
    "assert X_train_emb.shape[0] == y_train_emb.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dell-3060/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 24, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 386       \n",
      "=================================================================\n",
      "Total params: 80,386\n",
      "Trainable params: 80,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_model = models.Sequential()\n",
    "emb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
    "emb_model.add(layers.Flatten())\n",
    "emb_model.add(layers.Dense(2, activation='softmax'))\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dell-3060/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/20\n",
      "129251/129251 [==============================] - 1s 5us/step - loss: 0.2957 - acc: 0.9056 - val_loss: 0.1843 - val_acc: 0.9348\n",
      "Epoch 2/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1613 - acc: 0.9410 - val_loss: 0.1511 - val_acc: 0.9478\n",
      "Epoch 3/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1402 - acc: 0.9503 - val_loss: 0.1449 - val_acc: 0.9520\n",
      "Epoch 4/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1325 - acc: 0.9535 - val_loss: 0.1436 - val_acc: 0.9524\n",
      "Epoch 5/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1284 - acc: 0.9550 - val_loss: 0.1435 - val_acc: 0.9533\n",
      "Epoch 6/20\n",
      "129251/129251 [==============================] - 1s 5us/step - loss: 0.1253 - acc: 0.9558 - val_loss: 0.1438 - val_acc: 0.9544\n",
      "Epoch 7/20\n",
      "129251/129251 [==============================] - 1s 4us/step - loss: 0.1224 - acc: 0.9570 - val_loss: 0.1436 - val_acc: 0.9540\n",
      "Epoch 8/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1194 - acc: 0.9577 - val_loss: 0.1437 - val_acc: 0.9540\n",
      "Epoch 9/20\n",
      "129251/129251 [==============================] - 1s 4us/step - loss: 0.1163 - acc: 0.9586 - val_loss: 0.1448 - val_acc: 0.9541\n",
      "Epoch 10/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1131 - acc: 0.9596 - val_loss: 0.1445 - val_acc: 0.9538\n",
      "Epoch 11/20\n",
      "129251/129251 [==============================] - 1s 4us/step - loss: 0.1097 - acc: 0.9609 - val_loss: 0.1453 - val_acc: 0.9536\n",
      "Epoch 12/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1062 - acc: 0.9621 - val_loss: 0.1463 - val_acc: 0.9537\n",
      "Epoch 13/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.1026 - acc: 0.9633 - val_loss: 0.1468 - val_acc: 0.9540\n",
      "Epoch 14/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.0990 - acc: 0.9646 - val_loss: 0.1484 - val_acc: 0.9536\n",
      "Epoch 15/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.0955 - acc: 0.9660 - val_loss: 0.1502 - val_acc: 0.9538\n",
      "Epoch 16/20\n",
      "129251/129251 [==============================] - 1s 4us/step - loss: 0.0920 - acc: 0.9672 - val_loss: 0.1514 - val_acc: 0.9534\n",
      "Epoch 17/20\n",
      "129251/129251 [==============================] - 1s 5us/step - loss: 0.0886 - acc: 0.9685 - val_loss: 0.1535 - val_acc: 0.9527\n",
      "Epoch 18/20\n",
      "129251/129251 [==============================] - 1s 4us/step - loss: 0.0854 - acc: 0.9696 - val_loss: 0.1560 - val_acc: 0.9519\n",
      "Epoch 19/20\n",
      "129251/129251 [==============================] - 1s 4us/step - loss: 0.0824 - acc: 0.9708 - val_loss: 0.1582 - val_acc: 0.9520\n",
      "Epoch 20/20\n",
      "129251/129251 [==============================] - 0s 4us/step - loss: 0.0795 - acc: 0.9720 - val_loss: 0.1600 - val_acc: 0.9511\n"
     ]
    }
   ],
   "source": [
    "emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(emb_history, 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(emb_history, 'loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_results = test_model(emb_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 6)\n",
    "print('/n')\n",
    "print('Test accuracy of word embeddings model: {0:.2f}%'.format(emb_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.twitter.27B.25d.txt'\n",
    "glove_dir = 'glove/'\n",
    "emb_dict = {}\n",
    "glove = open(input_path / glove_dir / glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_words = ['fuck', 'pussy', 'sad', 'hell']\n",
    "for w in airline_words:\n",
    "    if w in emb_dict.keys():\n",
    "        print('Found the word {} in the dictionary'.format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIM = 25\n",
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    # The word_index contains a token for all words of the training data so we need to limit that\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "        # Otherwise the vector is kept with only zeros\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = models.Sequential()\n",
    "glove_model.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "glove_model.add(layers.Flatten())\n",
    "glove_model.add(layers.Dense(2, activation='softmax'))\n",
    "glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_history = deep_model(glove_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(glove_history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(glove_history, 'acc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_results = test_model(glove_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\n",
    "print('/n')\n",
    "print('Test accuracy of word glove model: {0:.2f}%'.format(glove_results[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Training word embeddings with more dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM \n",
    "lstm_out = 20\n",
    "\n",
    "emb_model2 = models.Sequential()\n",
    "emb_model2.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "emb_model2.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "emb_model2.add(layers.Dense(2, activation='softmax'))\n",
    "emb_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_history2 = deep_model(emb_model2, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(emb_history2, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(emb_history2, 'acc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_results2 = test_model(emb_model2, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\n",
    "# emb_results2.save(\"./output/model/model.h5\")\n",
    "print('/n')\n",
    "print('Test accuracy of word embedding model 2: {0:.2f}%'.format(emb_results2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twt = [\"love you\"]\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tk.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=24, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = emb_model2.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "print(sentiment)\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"positive\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
